{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action=\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import sweetviz as sv\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mytransformer as myt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the log for this Notebook\n",
    "now = datetime.now()\n",
    "filename = 'logs/08-finalize-model' + now.strftime(\"%m%d%Y\") + '.log'\n",
    "\n",
    "logging.basicConfig(filename=filename ,format='%(asctime)s | %(levelname)s: %(message)s', level=20)\n",
    "logging.info('')\n",
    "logging.info('***************************************************************************************************')\n",
    "logging.info('***                                                                                             ***')\n",
    "logging.info('***   NEW RUN                                                                                   ***')\n",
    "logging.info('***                                                                                             ***')\n",
    "logging.info('***************************************************************************************************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8902, 67)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# folder = '../../data source/_anonymized/'\n",
    "# declare some constants\n",
    "folder = '_anonymized/'\n",
    "sweetviz = True\n",
    "\n",
    "# read the cleaned csv files\n",
    "\n",
    "df = pd.read_csv(folder + 'dataset_4_modelling.csv')\n",
    "try:\n",
    "    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "except:\n",
    "    logging.warning('Unnamed: 0 not existing in this file.')\n",
    "    \n",
    "logging.info('{} rows and {} columns read in the CSV file'.format(df.shape[0], df.shape[1]))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cols': ['NET_FLOWS', 'AUM'],\n",
       "  'method': 'shift',\n",
       "  'period': [2, 3, 4],\n",
       "  'original': False},\n",
       " {'cols': ['IN_FLOWS'],\n",
       "  'method': 'expanding',\n",
       "  'period': [2, 4],\n",
       "  'original': False},\n",
       " {'cols': ['BENCH_PERF_SLOPE'],\n",
       "  'method': 'rolling',\n",
       "  'period': [2, 4],\n",
       "  'original': False},\n",
       " {'cols': ['EVENT_IMPACT'],\n",
       "  'method': 'ewm',\n",
       "  'period': [2, 3],\n",
       "  'original': False}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans=[]\n",
    "dico= {}\n",
    "dico['cols'] = ['NET_FLOWS','AUM']\n",
    "dico['method'] = 'shift'\n",
    "dico['period'] = [2,3,4]\n",
    "dico['original'] = False\n",
    "trans.append(dico)\n",
    "dico= {}\n",
    "dico['cols'] = ['IN_FLOWS']\n",
    "dico['method'] = 'expanding'\n",
    "dico['period'] = [2,4]\n",
    "dico['original'] = False\n",
    "trans.append(dico)\n",
    "dico= {}\n",
    "dico['cols'] = ['BENCH_PERF_SLOPE']\n",
    "dico['method'] = 'rolling'\n",
    "dico['period'] = [2,4]\n",
    "dico['original'] = False\n",
    "trans.append(dico)\n",
    "dico= {}\n",
    "dico['cols'] = ['EVENT_IMPACT']\n",
    "dico['method'] = 'ewm'\n",
    "dico['period'] = [2,3]\n",
    "dico['original'] = False\n",
    "trans.append(dico)\n",
    "\n",
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS2SL(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols=None, transformation=None):\n",
    "\n",
    "        self.cols=cols\n",
    "        self.transformation=transformation\n",
    "        \n",
    "    #def preprocess_f(self, X_df, train_mean):\n",
    "    def preprocess_f(self, X_df):\n",
    "        # Work on a copy\n",
    "        #print(self.cols)\n",
    "        X_df = X_df[self.cols].copy()\n",
    "        b_breakdown = np.unique(X_df['SRC_UID'])\n",
    "        # Missing values in continuous features\n",
    "        for trans in self.transformation:\n",
    "            dico = trans\n",
    "            for i, (k, v) in enumerate(dico.items()):\n",
    "                logging.info('index {} - dico value for key {} is {}'.format(i, k, v))            \n",
    "                # enumerate dictionary key and value\n",
    "\n",
    "            for b in b_breakdown:\n",
    "                f=X_df['SRC_UID']==b\n",
    "                all_cols = dico['cols'].copy()\n",
    "\n",
    "                for col in all_cols:\n",
    "                    for p in dico['period']:\n",
    "                        #print('change ',col,'p=',p)\n",
    "                        if dico['method']=='shift':   \n",
    "                            m='s'\n",
    "                            X_df.loc[f,col+'_s_'+str(p)]=X_df.loc[f,col].shift(periods=p)\n",
    "                        if dico['method']=='rolling': \n",
    "                            m='r'\n",
    "                            X_df.loc[f,col+'_r_'+str(p)]=X_df.loc[f,col].rolling(p).mean()\n",
    "                        if dico['method']=='expanding':   \n",
    "                            m='x'\n",
    "                            X_df.loc[f,col+'_x_'+str(p)]=X_df.loc[f,col].expanding(min_periods=p).mean()\n",
    "                        if dico['method']=='ewm':  \n",
    "                            m='e'\n",
    "                            X_df.loc[f,col+'_e_'+str(p)]=X_df.loc[f,col].ewm(com=p).mean()\n",
    "\n",
    "\n",
    "                        X_df.loc[f,col+'_'+m+'_'+str(p)]=X_df.loc[f,col+'_'+m+'_'+str(p)].interpolate(limit_direction='both')    \n",
    "\n",
    "            for c in all_cols:\n",
    "                X_df.drop(c,axis=1,inplace=True)\n",
    "\n",
    "        X_df.drop('TARGET',axis=1,inplace=True)\n",
    "\n",
    "        \n",
    "        print('return X_df', X_df.shape)\n",
    "        return X_df.fillna(0)\n",
    "\n",
    "    def fit(self, X_df, y=None):\n",
    "        # Check that we get a DataFrame\n",
    "        \n",
    "        assert type(X_df) == pd.DataFrame\n",
    "        print('arriving in fit',X_df.shape)\n",
    "\n",
    "        \n",
    "        X_preprocessed = self.preprocess_f(X_df)\n",
    "\n",
    "        # Save columns names/order for inference time\n",
    "        self.columns_ = X_preprocessed.columns\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        # Check that we get a DataFrame\n",
    "        assert type(X_df) == pd.DataFrame\n",
    "\n",
    "        # Preprocess data\n",
    "\n",
    "        X_preprocessed = self.preprocess_f(X_df)\n",
    "\n",
    "        # Make sure to have the same features\n",
    "        X_reindexed = X_preprocessed.reindex(columns=self.columns_, fill_value=0)\n",
    "\n",
    "        return X_reindexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c=['SRC_UID','TARGET','IN_FLOWS','NET_FLOWS','OUT_FLOWS','AUM','IS_STRATEGIC','BENCH_PERF_SLOPE','EVENT_IMPACT']\n",
    "temp_df=df[c].copy()\n",
    "preprocessor = TS2SL(cols=c, transformation=trans)\n",
    "preprocessor.fit(temp_df)\n",
    "preprocessor.transform(temp_df.iloc[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arriving in fit (6231, 9)\n",
      "return X_df (6231, 15)\n",
      "return X_df (6231, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\resqu\\.conda\\envs\\exts-ml\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\resqu\\.conda\\envs\\exts-ml\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return X_df (2671, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\resqu\\.conda\\envs\\exts-ml\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4672407338075627, (2671, 9), (2671,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=['SRC_UID','TARGET','IN_FLOWS','NET_FLOWS','OUT_FLOWS','AUM','IS_STRATEGIC','BENCH_PERF_SLOPE','EVENT_IMPACT']\n",
    "\n",
    "\n",
    "# Use our custom transformer in a pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', TS2SL(cols=c, transformation=trans)),\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('estimator', LogisticRegression())\n",
    "])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "\n",
    "# Split data\n",
    "X = df[c].drop('TARGET', axis=1)\n",
    "X = df[c]\n",
    "y = df.TARGET\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Evaluate estimator\n",
    "pipe.fit(X_tr, y_tr)\n",
    "\n",
    "pipe.score(X_te, y_te), X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessor', TS2SL(cols=['SRC_UID', 'TARGET', 'IN_FLOWS', 'NET_FLOWS', 'OUT_FLOWS', 'AUM', 'IS_STRATEGIC', 'BENCH_PERF_SLOPE', 'EVENT_IMPACT'],\n",
       "   transformation=[{'cols': ['NET_FLOWS', 'AUM'], 'method': 'shift', 'period': [2, 3, 4], 'original': False}, {'cols': ['IN_FLOWS'], 'method': 'e...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return X_df (3, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\resqu\\.conda\\envs\\exts-ml\\lib\\site-packages\\sklearn\\pipeline.py:331: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  0, -1], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(X_te[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SRC_UID</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>IN_FLOWS</th>\n",
       "      <th>NET_FLOWS</th>\n",
       "      <th>OUT_FLOWS</th>\n",
       "      <th>AUM</th>\n",
       "      <th>IS_STRATEGIC</th>\n",
       "      <th>BENCH_PERF_SLOPE</th>\n",
       "      <th>EVENT_IMPACT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8090</th>\n",
       "      <td>189669396589741</td>\n",
       "      <td>0</td>\n",
       "      <td>348112</td>\n",
       "      <td>246030</td>\n",
       "      <td>-102082</td>\n",
       "      <td>148494034</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>2.496147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2554</th>\n",
       "      <td>323418458</td>\n",
       "      <td>1</td>\n",
       "      <td>26382634</td>\n",
       "      <td>12139507</td>\n",
       "      <td>-14243127</td>\n",
       "      <td>604148860</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.004198</td>\n",
       "      <td>32.643325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>108202450315</td>\n",
       "      <td>1</td>\n",
       "      <td>177749960</td>\n",
       "      <td>97903022</td>\n",
       "      <td>-79846938</td>\n",
       "      <td>843190921</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>7.976262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              SRC_UID  TARGET   IN_FLOWS  NET_FLOWS  OUT_FLOWS        AUM  \\\n",
       "8090  189669396589741       0     348112     246030    -102082  148494034   \n",
       "2554        323418458       1   26382634   12139507  -14243127  604148860   \n",
       "4753     108202450315       1  177749960   97903022  -79846938  843190921   \n",
       "\n",
       "      IS_STRATEGIC  BENCH_PERF_SLOPE  EVENT_IMPACT  \n",
       "8090             0          0.001688      2.496147  \n",
       "2554             0         -0.004198     32.643325  \n",
       "4753             0          0.000813      7.976262  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define de columns based on business experience\n",
    "cols_flows = ['IN_FLOWS', 'OUT_FLOWS','AUM']\n",
    "cols_perfs = ['BENCH_PERF_SLOPE', 'BENCH_PERF_SLOPE_ERROR',\n",
    "       'BENCH_VOL_SLOPE', 'BENCH_VOL_SLOPE_ERROR', 'NAV_PERF_SLOPE',\n",
    "       'NAV_PERF_SLOPE_ERROR', 'NAV_VOL_SLOPE', 'NAV_VOL_SLOPE_ERROR']\n",
    "cols_maktg = ['POSITIVE_RFP', 'EVENT_IMPACT', 'DOWNLOADED_DOCUMENTS','PSU_SCORE']\n",
    "cols_funds = ['IS_STRATEGIC', 'ADMINSTRATION_FEES', 'MANAGEMENT_FEES','RISK_LEVEL_VALUE','STRATEGY_CAPABILITY','FUND_AGE']\n",
    "\n",
    "all_cols =  cols_flows + cols_perfs + cols_maktg + cols_funds + ['TARGET', 'FLOWS_YEAR', 'FLOWS_MONTH', 'SRC_UID']\n",
    "# create the transformer parameter\n",
    "trans = []\n",
    "for i in range(len(cols_flows)):\n",
    "    dico= {}\n",
    "    dico['cols'] = [cols_flows[i]]\n",
    "    dico['method'] = 'rolling'\n",
    "    dico['period'] = [2,3,6]\n",
    "    dico['original'] = False\n",
    "    trans.append(dico)\n",
    "    \n",
    "\n",
    "for i in range(len(cols_perfs)):\n",
    "    dico= {}\n",
    "    dico['cols'] = [cols_perfs[i]]\n",
    "    dico['method'] = 'shift'\n",
    "    dico['period'] = [2,3,5]\n",
    "    dico['original'] = False\n",
    "    trans.append(dico)\n",
    "    \n",
    "for i in range(len(cols_maktg)):\n",
    "    dico= {}\n",
    "    dico['cols'] = [cols_maktg[i]]\n",
    "    dico['method'] = 'expanding'\n",
    "    dico['period'] = [3,6,9]\n",
    "    dico['original'] = False\n",
    "    trans.append(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arriving in fit (6231, 25)\n",
      "return X_df (6231, 54)\n",
      "return X_df (6231, 54)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\resqu\\.conda\\envs\\exts-ml\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\resqu\\.conda\\envs\\exts-ml\\lib\\site-packages\\sklearn\\base.py:467: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return X_df (2671, 54)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\resqu\\.conda\\envs\\exts-ml\\lib\\site-packages\\sklearn\\pipeline.py:511: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6098839385997754, (2671, 25), (2671,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=all_cols\n",
    "# Use our custom transformer in a pipeline\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', TS2SL(cols=c, transformation=trans)),\n",
    "    ('Scaler', StandardScaler()), ('Power', PowerTransformer()),\n",
    "    ('estimator', LogisticRegression(C=100,solver='lbfgs',class_weight='balanced', max_iter=1000))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Split data\n",
    "X = df[c].drop('TARGET', axis=1)\n",
    "X = df[c]\n",
    "y = df.TARGET\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Evaluate estimator\n",
    "pipe.fit(X_tr, y_tr)\n",
    "\n",
    "pipe.score(X_te, y_te), X_te.shape, y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'models/finalized_model.sav'\n",
    "pickle.dump(pipe, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'LR__C':[100,1000,10000],\n",
    "    'LR__solver':['newton-cg','liblinear','lbfgs'],#\n",
    "    'LR__class_weight':[None,'balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm_final\n",
    "for cv in tqdm_final([2,5,10]):\n",
    "    grid = GridSearchCV(pipe, param_grid = params,  cv=cv, verbose=1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(cv, grid.score(X_validation, y_validation))\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "# get the best estimator\n",
    "best_estimator = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
